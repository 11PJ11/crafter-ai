project_id: "des-us004"
created_at: "2026-01-27T17:15:00Z"
feature_description: "Implement audit trail for compliance verification - capture all state transitions with ISO timestamps, append-only immutable logging, event categorization, and daily log rotation"
baseline_reference: "docs/feature/des-us004/baseline.yaml"

phases:
  - phase_id: "01"
    name: "Foundation: Core Audit Logging Infrastructure"
    description: "Establish append-only audit logging with immutability guarantees and foundational data structures"
    steps:
      - step_id: "01-01"
        name: "Create AuditLogger class with append-only semantics"
        description: "Implement base AuditLogger class supporting append-only file operations with SHA256 content hash tracking for immutability verification. Serves as foundation for all audit trail functionality."
        acceptance_criteria:
          - "AuditLogger initializes or opens existing audit log file in append mode"
          - "Existing entries cannot be modified (file opened with 'a' mode, not 'w')"
          - "New entries appended to file with correct ordering preserved"
          - "File created at configurable path with appropriate permissions (readable)"
          - "SHA256 hash of existing entries computed before and after append operations"
          - "Content hash verification detects any tampering with existing entries"
        suggested_agent: "software-crafter"
        requires: []
        related_tests: ["test_scenario_002_audit_log_is_append_only_immutable"]
        implementation_notes: |
          Create src/des/adapters/driven/logging/audit_logger.py with:
          - AuditLogger class with __init__(log_path)
          - append(entry: dict) method that writes JSON line to file
          - read_entries() method that reads and parses all entries
          - compute_hash() method for SHA256 of existing entries
          - verify_immutability() method to check hash consistency
          - Use atomic file operations to prevent corruption
          - Store hash metadata in separate .hash file if needed for verification
        estimated_hours: 2.5
        acceptance_test_mapping: |
          Acceptance Test: test_scenario_002_audit_log_is_append_only_immutable
          - Creates AuditLogger with 5 initial entries
          - Computes SHA256 hash of first 5 entries
          - Appends 3 new entries
          - Verifies hash of original 5 entries unchanged
          - Confirms no entry modification possible

      - step_id: "01-02"
        name: "Implement ISO 8601 timestamp utility with millisecond precision"
        description: "Create timestamp utility providing consistent ISO 8601 formatted timestamps for all audit entries. Ensures compliance with timestamp format AC-004.2 across entire audit trail."
        acceptance_criteria:
          - "Timestamps formatted as ISO 8601 with Z timezone indicator (UTC)"
          - "Millisecond precision with exactly 3 decimal places (HH:MM:SS.sssZ format)"
          - "Consistent timezone handling - all times UTC internally"
          - "Validation function validates timestamp format compliance"
          - "Function can be used to format arbitrary datetime objects"
          - "Generated timestamps parseable by datetime.fromisoformat()"
        suggested_agent: "software-crafter"
        requires: ["01-01"]
        related_tests: ["test_scenario_001_state_transitions_logged_with_iso_timestamp"]
        implementation_notes: |
          Create src/des/adapters/timestamp_util.py with:
          - get_iso_timestamp() → str: Returns current UTC time in ISO format
            Example: "2026-01-27T17:15:45.123Z"
          - format_as_iso(dt: datetime) → str: Formats arbitrary datetime
          - validate_iso_format(ts: str) → bool: Validates compliance
          - Use datetime.now(timezone.utc).isoformat()
          - Replace '+00:00' with 'Z' for timezone indicator
          - Ensure milliseconds included via microsecond precision
          - Handle timezone-aware datetime conversions
        estimated_hours: 1.0
        acceptance_test_mapping: |
          Acceptance Test: test_scenario_001_state_transitions_logged_with_iso_timestamp
          - Calls get_iso_timestamp() multiple times
          - Verifies format: YYYY-MM-DDTHH:MM:SS.sssZ
          - Confirms parseable via datetime.fromisoformat()
          - Validates Z timezone marker present
          - Confirms millisecond precision (3 decimals)

      - step_id: "01-03"
        name: "Implement event type constants and AuditEvent dataclass"
        description: "Define event type constants and AuditEvent dataclass for structured event representation. Establishes 4 event categories (TASK_INVOCATION, PHASE, SUBAGENT_STOP, COMMIT) with subcategories for complete event taxonomy."
        acceptance_criteria:
          - "EventType enum with 4 top-level categories"
          - "Subcategories for each event type (STARTED, COMPLETED, FAILED, REJECTED, etc.)"
          - "AuditEvent dataclass with all required fields"
          - "Type validation enforces valid event types at creation time"
          - "Serialization to JSON-compatible format for file storage"
          - "All event types documented with examples"
          - "Dataclass supports immutability constraints"
        suggested_agent: "software-crafter"
        requires: []
        related_tests: ["test_scenario_003a_task_invocation_events_captured", "test_scenario_003b_phase_lifecycle_events_captured", "test_scenario_003c_subagent_stop_and_commit_events_captured"]
        implementation_notes: |
          Create src/des/adapters/event_types.py with:
          - EventType enum with variants:
            * TASK_INVOCATION_STARTED, TASK_INVOCATION_VALIDATED, TASK_INVOCATION_REJECTED
            * PHASE_STARTED, PHASE_EXECUTED, PHASE_SKIPPED, PHASE_FAILED
            * SUBAGENT_STOP_VALIDATION, SUBAGENT_STOP_FAILURE
            * COMMIT_SUCCESS, COMMIT_FAILURE
          - AuditEvent dataclass with fields:
            * timestamp: str (ISO format)
            * event: EventType
            * step_path: str
            * phase_name: Optional[str]
            * status: str (e.g., "success", "failure", "abandoned")
            * outcome: Optional[str]
            * duration_minutes: Optional[float]
            * failure_reason: Optional[str]
          - __post_init__ validation to enforce required fields
          - to_dict() for JSON serialization
          - from_dict() for deserialization
        estimated_hours: 1.5
        acceptance_test_mapping: |
          Acceptance Test: test_scenario_003a_task_invocation_events_captured
          - Verifies TASK_INVOCATION_* event types exist and creatable
          - Tests TASK_INVOCATION_STARTED, TASK_INVOCATION_VALIDATED events

          Acceptance Test: test_scenario_003b_phase_lifecycle_events_captured
          - Tests all PHASE_* event types (STARTED, EXECUTED, SKIPPED, FAILED)
          - Verifies 14 phases can each generate correct event types

          Acceptance Test: test_scenario_003c_subagent_stop_and_commit_events_captured
          - Tests SUBAGENT_STOP_VALIDATION, COMMIT_SUCCESS, COMMIT_FAILURE types
          - Confirms all event variants exist and creatable

  - phase_id: "02"
    name: "Event Logging: Implement DES Event Capture"
    description: "Implement event capture hooks at orchestrator and phase execution points to log all state transitions"
    steps:
      - step_id: "02-01"
        name: "Implement TASK_INVOCATION event logging"
        description: "Capture task invocation start and validation completion events when /nw:execute command is invoked. Integrates with DESOrchestrator.render_prompt() entry point to log initiation and validation completion."
        acceptance_criteria:
          - "TASK_INVOCATION_STARTED logged when task execution begins"
          - "TASK_INVOCATION_VALIDATED logged immediately after validation passes"
          - "TASK_INVOCATION_REJECTED logged if validation fails with error details"
          - "Events contain step_path, timestamp, and execution context"
          - "Integration point in DESOrchestrator.render_prompt() captures entry event"
          - "Integration point after validation captures success/failure event"
          - "Task rejection reason captured in failure event"
        suggested_agent: "software-crafter"
        requires: ["01-01", "01-02", "01-03"]
        related_tests: ["test_scenario_003a_task_invocation_events_captured"]
        implementation_notes: |
          Modify src/des/orchestrator.py:
          - Initialize audit_logger in __init__
          - In render_prompt() - ADD at START:
            * Create TASK_INVOCATION_STARTED event
            * Log with step_path from task file
            * Call audit_logger.append()
          - In render_prompt() - ADD after validation:
            * If validation succeeds: TASK_INVOCATION_VALIDATED
            * If validation fails: TASK_INVOCATION_REJECTED with error_reason
            * Call audit_logger.append()
          - Pass validation result and error context to event
          - Handle cases where validation is deferred/asynchronous
        estimated_hours: 3.0
        acceptance_test_mapping: |
          Acceptance Test: test_scenario_003a_task_invocation_events_captured
          - Invokes /nw:execute for test task
          - Verifies TASK_INVOCATION_STARTED event logged with timestamp
          - Verifies TASK_INVOCATION_VALIDATED event logged on success
          - Verifies both events in audit trail
          - Validates event ordering (STARTED before VALIDATED)

      - step_id: "02-02"
        name: "Implement PHASE lifecycle event logging"
        description: "Log all 14 TDD phase transitions (PREPARE through COMMIT) with PHASE_STARTED, PHASE_EXECUTED, PHASE_SKIPPED events. Captures complete phase execution lifecycle integrated with existing phase_execution_log structure."
        acceptance_criteria:
          - "PHASE_STARTED logged when each of 14 TDD phases begins"
          - "PHASE_EXECUTED or PHASE_SKIPPED logged at phase completion"
          - "All 14 TDD phases covered (PREPARE, RESEARCH, RED_UNIT, ... , COMMIT)"
          - "Phase name and sequential order captured in events"
          - "Skip reason captured when phase is skipped"
          - "Phase duration calculated and stored"
          - "Integration with phase_execution_log in step file for consistency"
        suggested_agent: "software-crafter"
        requires: ["02-01"]
        related_tests: ["test_scenario_003b_phase_lifecycle_events_captured"]
        implementation_notes: |
          Modify src/des/orchestrator.py phase execution logic:
          - Before executing each phase:
            * Create PHASE_STARTED event with phase_name
            * Log current phase number (1-14) in context
            * Call audit_logger.append()
          - After phase execution:
            * If phase executed successfully: PHASE_EXECUTED
            * If phase skipped: PHASE_SKIPPED with skip_reason
            * If phase failed: PHASE_FAILED with error context
            * Include phase duration (start_time to end_time)
            * Call audit_logger.append()
          - Ensure phase names match TDD phase taxonomy (from phase_execution_log)
          - Handle early termination (e.g., stop after RED_UNIT on failure)
        estimated_hours: 4.0
        acceptance_test_mapping: |
          Acceptance Test: test_scenario_003b_phase_lifecycle_events_captured
          - Executes complete task workflow through all 14 phases
          - Verifies 14 PHASE_STARTED events in audit trail
          - Verifies 14 PHASE_EXECUTED or PHASE_SKIPPED events
          - Validates correct phase names in events
          - Confirms chronological ordering of phase events
          - Total of ~28 phase-related events for complete execution

      - step_id: "02-03"
        name: "Implement SUBAGENT_STOP and COMMIT event logging"
        description: "Log subagent completion and git commit events capturing termination validation results and commit success/failure. Integrates with SubagentStopHook and git post-commit detection."
        acceptance_criteria:
          - "SUBAGENT_STOP_VALIDATION logged when subagent execution terminates"
          - "Validation result (success/failure) captured in event"
          - "COMMIT_SUCCESS or COMMIT_FAILURE logged after git commit"
          - "Commit hash recorded in event on successful commit"
          - "Commit failure reason (e.g., nothing to commit, merge conflict) captured"
          - "Integration with SubagentStopHook for termination detection"
          - "Integration with git post-commit hook for commit detection"
        suggested_agent: "software-crafter"
        requires: ["02-02"]
        related_tests: ["test_scenario_003c_subagent_stop_and_commit_events_captured"]
        implementation_notes: |
          Modify src/des/orchestrator.py and create hook integrations:
          - Add audit logging to SubagentStopHook (or create new audit-aware hook):
            * When subagent stops: Create SUBAGENT_STOP_VALIDATION event
            * Include validation_status (success/failure)
            * Include stop_reason if applicable
            * Call audit_logger.append()
          - Add audit logging to git commit detection:
            * After commit attempt, check result
            * If commit succeeds: COMMIT_SUCCESS with commit_hash
            * If commit fails (nothing to commit): COMMIT_FAILURE with reason="nothing_to_commit"
            * If commit fails (other): COMMIT_FAILURE with reason and error_message
            * Call audit_logger.append()
          - Handle edge cases (detached HEAD, merge state, etc.)
        estimated_hours: 3.0
        acceptance_test_mapping: |
          Acceptance Test: test_scenario_003c_subagent_stop_and_commit_events_captured
          - Executes task through completion with commit
          - Verifies SUBAGENT_STOP_VALIDATION event logged
          - Verifies COMMIT_SUCCESS or COMMIT_FAILURE event logged
          - Confirms commit hash in success event
          - Validates event timestamps and context

      - step_id: "02-04"
        name: "Implement rejection and failure event logging"
        description: "Capture validation rejections and phase failures with detailed rejection reasons and diagnostic context. Enables post-mortem analysis and debugging of failed executions."
        acceptance_criteria:
          - "VALIDATION_REJECTED logged for failed validation at any point"
          - "PHASE_FAILED logged for phase failures or errors"
          - "Rejection reason captured with specific error details"
          - "Failure context included (line number, assertion message, traceback snippet)"
          - "Debugging information (variable state, input values) available"
          - "Failure timestamp and duration recorded"
          - "Events enable reconstruction of failure scenario"
        suggested_agent: "software-crafter"
        requires: ["02-03"]
        related_tests: ["test_scenario_004_failed_validation_captures_rejection_event"]
        implementation_notes: |
          Enhance audit logging for failure scenarios:
          - In validation failure paths:
            * Catch validation exceptions
            * Create VALIDATION_REJECTED event
            * Include error message, type, traceback
            * Include relevant context (input data, assertion that failed)
            * Call audit_logger.append()
          - In phase failure paths:
            * Catch phase execution exceptions
            * Create PHASE_FAILED event
            * Include phase_name, failure_reason
            * Include line number if from test output
            * Include assertion message if applicable
            * Call audit_logger.append()
          - Ensure no sensitive data (passwords, tokens) in failure context
          - Test with intentional validation and phase failures
        estimated_hours: 2.5
        acceptance_test_mapping: |
          Acceptance Test: test_scenario_004_failed_validation_captures_rejection_event
          - Triggers validation failure intentionally
          - Verifies VALIDATION_REJECTED event logged
          - Confirms rejection reason captured
          - Validates failure context includes error details
          - Ensures event enables debugging of failure

  - phase_id: "03"
    name: "Advanced Features: Immutability, Context, Rotation, Integration"
    description: "Implement advanced audit trail features including context enrichment, crash recovery support, format compliance, and daily rotation"
    steps:
      - step_id: "03-01"
        name: "Implement entry context enrichment"
        description: "Enhance audit entries with complete execution context including step_path, phase_name, status, outcome, duration, and failure reason. Ensures all audit entries are self-contained and traceably without external references."
        acceptance_criteria:
          - "All 6 context fields present in every entry"
          - "step_path identifies which step file is executing"
          - "phase_name identifies which of 14 TDD phases (PREPARE, RED_UNIT, etc.)"
          - "status field indicates phase state (IN_PROGRESS, EXECUTED, SKIPPED, ABANDONED)"
          - "outcome field records result (success, failure, skipped)"
          - "duration_minutes field calculated from start to end"
          - "failure_reason field populated when outcome=failure"
          - "Entry structure consistent across all event types"
        suggested_agent: "software-crafter"
        requires: ["02-04"]
        related_tests: ["test_scenario_005_audit_entries_include_step_path_and_event_data"]
        implementation_notes: |
          Modify AuditEvent dataclass and all logging sites:
          - Update AuditEvent to include all context fields:
            * step_path: str (required)
            * phase_name: Optional[str] (required if phase context exists)
            * status: str (required)
            * outcome: str (required)
            * duration_minutes: Optional[float]
            * failure_reason: Optional[str]
          - Update all audit_logger.append() calls to pass complete context:
            * At task invocation: step_path from task
            * At phase start: phase_name from current phase
            * At phase end: duration calculated, outcome set
            * On failure: failure_reason populated with error
          - Add context validator to ensure required fields populated
          - Test that all entry types have complete context
        estimated_hours: 2.0
        acceptance_test_mapping: |
          Acceptance Test: test_scenario_005_audit_entries_include_step_path_and_event_data
          - Executes task and reads audit entries
          - Validates all 6 context fields present in each entry
          - Confirms step_path correct for task being executed
          - Confirms phase_name correct for current phase
          - Validates status, outcome, duration populated
          - Verifies entry structure self-contained

      - step_id: "03-02"
        name: "Implement crash recovery audit trail"
        description: "Detect and log abandoned phases (agent crash recovery scenarios) with explicit crash markers in audit trail. Enables post-crash analysis of failure point and recovery state."
        acceptance_criteria:
          - "Abandoned phases detected when PHASE_STARTED logged but no PHASE_COMPLETED"
          - "CRASH marker added to abandoned phase entry in audit trail"
          - "Recovery context captured (last successful phase, crash timestamp)"
          - "Audit trail traceable through entire crash scenario"
          - "Supports crash recovery workflow (identify failure point)"
          - "Crash detection integrated with SubagentStopHook"
          - "Recovery timestamp recorded when execution restarts"
        suggested_agent: "software-crafter"
        requires: ["03-01"]
        related_tests: ["test_scenario_006_abandoned_phase_traceable_in_audit"]
        implementation_notes: |
          Enhance audit trail for crash detection:
          - In SubagentStopHook or equivalent:
            * When subagent stops unexpectedly (without normal completion)
            * Check audit trail for PHASE_STARTED events without matching PHASE_COMPLETED
            * For each abandoned phase: Add CRASH marker
            * Log SUBAGENT_CRASH_DETECTED event with:
              - last_completed_phase
              - abandoned_phase
              - crash_timestamp
          - Integrate crash detection on next execution start:
            * Scan audit trail on startup
            * Identify abandoned phases from previous run
            * Log CRASH_RECOVERY_START with recovery_point
          - Create helper method to detect incomplete phases
          - Ensure crash detection doesn't require manual intervention
        estimated_hours: 2.5
        acceptance_test_mapping: |
          Acceptance Test: test_scenario_006_abandoned_phase_traceable_in_audit
          - Simulates agent crash during phase execution
          - Verifies PHASE_STARTED logged but PHASE_COMPLETED missing
          - Confirms CRASH marker added to audit trail
          - Validates recovery context captured
          - Confirms audit enables identification of failure point

      - step_id: "03-03"
        name: "Implement JSONL format output and readability"
        description: "Convert audit log to JSONL format (one JSON object per line) with human-readable field names and proper escaping. Enables both human review in text editor and programmatic parsing."
        acceptance_criteria:
          - "One audit entry per line in file (JSONL format)"
          - "Each line is valid, independently parseable JSON"
          - "Field names are human-readable (not abbreviated or coded)"
          - "Proper string escaping for special characters (newlines, quotes)"
          - "File is human-readable when opened in text editor"
          - "Standard JSONL parsers can read file successfully"
          - "No extra whitespace or formatting that breaks line parsing"
        suggested_agent: "software-crafter"
        requires: ["03-02"]
        related_tests: ["test_scenario_007_audit_log_is_human_readable_jsonl"]
        implementation_notes: |
          Modify AuditLogger to output JSONL format:
          - Update append() method:
            * Convert entry to dict (use to_dict() if dataclass)
            * Serialize with json.dumps(entry, separators=(',', ':'))
            * Write serialized JSON + '\n' (newline at end)
            * Do NOT add any other formatting (no indentation, no extra spaces)
          - Ensure field naming is human-readable:
            * 'timestamp' not 'ts'
            * 'step_path' not 'sp'
            * 'phase_name' not 'pn'
            * 'failure_reason' not 'reason' or 'fr'
          - Use compact JSON (no pretty-printing) to keep lines concise
          - Test readability: open file in text editor, read lines manually
          - Validate: python3 -c "import json; [json.loads(line) for line in open(f).readlines()]"
        estimated_hours: 1.5
        acceptance_test_mapping: |
          Acceptance Test: test_scenario_007_audit_log_is_human_readable_jsonl
          - Generates audit trail for task
          - Opens audit log file
          - Validates each line is valid JSON
          - Confirms field names are human-readable
          - Parses file with json.loads() for each line
          - Verifies human can read file in text editor

      - step_id: "03-04"
        name: "Implement daily log rotation"
        description: "Automatically rotate audit logs daily with date-based file naming (audit-YYYY-MM-DD.log) to prevent single file bloat. Enables efficient log management and date-based discovery."
        acceptance_criteria:
          - "New log file created at midnight UTC (00:00:00Z)"
          - "File naming format: audit-YYYY-MM-DD.log (e.g., audit-2026-01-27.log)"
          - "Old entries preserved in previous day's file"
          - "Continuous operation across midnight boundary (no data loss)"
          - "Support querying logs by date"
          - "Each daily file remains manageable size (~600 entries = ~30KB)"
          - "Rotation happens automatically without manual intervention"
        suggested_agent: "software-crafter"
        requires: ["03-03"]
        related_tests: ["test_scenario_008_audit_logs_rotate_daily_with_date_naming"]
        implementation_notes: |
          Implement LogRotationManager class:
          - Create src/des/adapters/log_rotation_manager.py
          - Constructor takes base_log_directory
          - Method get_current_log_path() returns audit-YYYY-MM-DD.log
          - Check current date each time append() called
          - If date changed: close old file, open new file
          - Persist date separately to detect midnight transitions
          - Handle edge cases:
            * System time change (backward or forward)
            * Timezone edge cases (UTC-based only)
            * Concurrent access to different day's logs
          - Integrate into AuditLogger:
            * Use LogRotationManager to get current file path
            * Call manager before each append to handle rotation
            * Update file handle if rotation occurred
        estimated_hours: 2.0
        acceptance_test_mapping: |
          Acceptance Test: test_scenario_008_audit_logs_rotate_daily_with_date_naming
          - Generates audit entries across multiple days
          - Verifies files named audit-YYYY-MM-DD.log correctly
          - Confirms entries distributed to correct daily files
          - Validates no entries lost across rotation boundary
          - Checks each daily file has expected entries

      - step_id: "03-05"
        name: "Implement scale testing (100 executions)"
        description: "Integration test with 100 execution cycles to verify audit trail reliability and file management at scale. Validates that system handles extended operation without degradation."
        acceptance_criteria:
          - "100 consecutive executions complete without errors"
          - "Log files rotate correctly (5-6 daily files for 100 executions)"
          - "Total file size remains manageable (~5-10 MB for ~3000 entries)"
          - "No data loss across 100 executions (~3000 entries total)"
          - "Query performance acceptable (< 1 second to search 100 files)"
          - "Archive strategy validated (old files preserved, new files created)"
          - "Memory usage stable across iterations (no memory leaks)"
        suggested_agent: "software-crafter"
        requires: ["03-04"]
        related_tests: ["test_scenario_009_daily_rotation_prevents_single_file_bloat"]
        implementation_notes: |
          Create tests/integration/test_us004_scale.py:
          - Fixture: configure test audit directory
          - Test: execute_100_cycles()
            * Loop 100 times:
              - Simulate complete DES execution
              - Generate ~30 audit entries per cycle
              - Verify no errors
            * After loop complete:
              - Count daily log files (expect 5-6)
              - Calculate total entries (~3000)
              - Measure total file size (expect ~5-10 MB)
              - Verify no missing entries
              - Run search query across all files (< 1s)
          - Monitor metrics:
            * Execution time per cycle (should be stable)
            * File sizes per day (should be consistent)
            * Total entries count
          - Generate report with results
        estimated_hours: 3.0
        acceptance_test_mapping: |
          Acceptance Test: test_scenario_009_daily_rotation_prevents_single_file_bloat
          - Runs 100 simulated executions
          - Verifies daily rotation distributes entries
          - Confirms no single file bloat
          - Validates manageable file sizes
          - Confirms all entries preserved
          - Measures performance metrics

      - step_id: "03-06"
        name: "Final integration and end-to-end validation"
        description: "Comprehensive integration test covering all 12 acceptance scenarios with complete validation, mutation testing, and regression testing. Validates complete audit trail implementation readiness."
        acceptance_criteria:
          - "All 12 acceptance tests passing (GREEN state)"
          - "Audit trail complete and accurate for all scenarios"
          - "Mutation testing score >= 75%"
          - "No regression in existing tests (553+ tests still passing)"
          - "Performance within budget (< 100ms per audit entry)"
          - "Documentation complete (code comments, docstrings, README)"
          - "Code coverage > 80% for audit_logger module"
          - "Integration with DES orchestrator validated end-to-end"
        suggested_agent: "software-crafter"
        requires: ["03-05"]
        related_tests: ["test_scenario_010_complete_execution_produces_reviewable_audit_trail"]
        implementation_notes: |
          Create comprehensive integration test:
          - Test: test_complete_audit_trail_integration()
            * Execute complete workflow (all 14 phases)
            * Verify all event types logged correctly
            * Validate immutability (no entry modifications)
            * Check daily rotation (if crossing midnight)
            * Verify JSONL format compliance
            * Run mutation testing on audit_logger.py
            * Run regression suite on existing tests
          - Validation checks:
            * Count total events (~30 expected)
            * Verify event ordering chronological
            * Check timestamp format compliance
            * Validate context completeness
            * Verify file permissions (readable)
          - Performance measurement:
            * Time to append entry (target < 100ms)
            * Time to read all entries (target < 1s)
            * Memory usage during execution
          - Coverage measurement:
            * Run pytest --cov on audit trail module
            * Target > 80% coverage
            * Identify and cover edge cases
          - Regression suite:
            * Run full test suite
            * Verify no test failures from audit changes
            * Confirm backwards compatibility
        estimated_hours: 2.0
        acceptance_test_mapping: |
          Acceptance Test: test_scenario_010_complete_execution_produces_reviewable_audit_trail
          - Executes complete workflow with audit logging
          - Reads generated audit trail
          - Validates all 12 scenarios' requirements met
          - Confirms audit trail enables PR review
          - Verifies compliance verification possible
          - Generates final integration report

summary:
  total_steps: 9
  total_estimated_hours: 31.0
  phases: 3
  critical_path: "01-01 → 01-02 → 01-03 → 02-01 → 02-02 → 02-03 → 02-04 → 03-01 → 03-02 → 03-03 → 03-04 → 03-05 → 03-06"

  phase_breakdown:
    phase_01_foundation:
      steps: 3
      hours: 5.0
      purpose: "Establish core audit infrastructure and data structures"
      quick_wins: ["01-01 (2.5h foundation)", "01-02 (1h utility)", "01-03 (1.5h types)"]

    phase_02_event_logging:
      steps: 4
      hours: 12.5
      purpose: "Implement event capture at all DES orchestrator points"
      dependencies: "Requires Phase 01 foundation"

    phase_03_advanced_features:
      steps: 6
      hours: 13.5
      purpose: "Implement context enrichment, crash recovery, format compliance, rotation"
      dependencies: "Requires Phase 02 events"

  implementation_approach: "Outside-In TDD with acceptance tests driving implementation"

  validation:
    status: "complete"
    notes: |
      Roadmap decomposes US-004 audit trail feature into 9 atomic, sequential steps across 3 phases:

      PHASE 01 (Foundation - 5h):
      - Establishes AuditLogger class with append-only semantics
      - Creates timestamp utility for ISO 8601 compliance
      - Defines event types and AuditEvent dataclass
      - Enables all subsequent event capture steps

      PHASE 02 (Event Logging - 12.5h):
      - Implements TASK_INVOCATION event capture
      - Implements PHASE lifecycle event logging (all 14 phases)
      - Implements SUBAGENT_STOP and COMMIT event logging
      - Implements failure and rejection event logging
      - Captures complete state transition history

      PHASE 03 (Advanced Features - 13.5h):
      - Enriches entries with complete execution context
      - Implements crash recovery detection and logging
      - Converts to JSONL format for readability
      - Implements daily log rotation with date naming
      - Validates at scale (100 executions)
      - Final integration and validation

      Each step targets one or more specific acceptance test scenarios, ensuring Outside-In TDD discipline.

      Quick wins establish foundation efficiently; advanced features build on solid base.

      All steps are atomic and can be implemented independently within their phase once dependencies met.

      Total effort: ~31 hours, distributed across 3 phases enabling parallel team work within phases.

implementation_guidelines:
  atdd_approach: "Each step maps to 1+ acceptance test scenarios. Implement until test passes, then move to next step."

  dependency_management: |
    - Phase 01 must complete before Phase 02 starts (foundation required)
    - Phase 02 steps 01→02→03→04 sequential (each depends on previous)
    - Phase 03 steps 01→02→03→04→05→06 sequential (each depends on previous)
    - Estimated parallelization: Teams can work on Phase 01 and Phase 02 steps in parallel after Phase 01 completes

  testing_strategy: |
    - Step: Implement feature
    - Verify: Run corresponding acceptance tests (should pass)
    - Validate: Run full test suite (confirm no regressions)
    - Measure: Collect metrics (coverage, performance, mutation score)
    - Iterate: Fix issues and re-verify before moving to next step

  code_quality: |
    - Maintain > 80% test coverage for audit trail module
    - Follow existing code style and conventions
    - Document all public APIs with docstrings
    - Include implementation comments for complex logic
    - Ensure backwards compatibility with DES orchestrator

  performance_targets: |
    - Audit entry append: < 100ms per entry
    - Audit entry read: < 1ms per entry
    - Daily file rotation: < 50ms detection + rotation
    - Scale test (100 executions): Total < 15 seconds
    - Memory usage: Stable across extended execution (no leaks)

rollback_strategy: |
  If step fails acceptance tests:
  1. Review test failure and understand root cause
  2. Fix implementation issues
  3. Re-verify tests pass before moving on
  4. DO NOT proceed to next step until current step fully passes
  5. If unfixable in reasonable time, escalate to architecture review
  6. Consider alternative implementation approach only after escalation

success_criteria:
  acceptance_tests: "All 12 acceptance tests pass (100%)"
  code_coverage: "> 80% for audit trail module"
  mutation_score: ">= 75%"
  regression_tests: "No regressions (all 553+ existing tests still pass)"
  performance: "All metrics within target (< 100ms per entry, < 1s for 100 file search)"
  integration: "Complete end-to-end audit trail working with DES orchestrator"
  documentation: "Code documented, README updated, examples provided"
  compliance: "Audit trail enables Priya to verify TDD compliance during PR review"

reviews:
  - reviewer: "software-crafter-reviewer"
    date: "2026-01-27T17:47:00Z"
    overall_assessment: "APPROVED"
    approval_status:
      ready_for_split: true
      blocking_issues: []
    review_summary: |
      Roadmap passes all approval criteria:

      ✓ Step atomicity: 9 atomic steps, each producing 1+ passing acceptance test
      ✓ Dependency accuracy: Valid DAG, correct ordering (01-01 → 01-02 → 01-03 → 02-01 → ... → 03-06)
      ✓ Effort realism: Phase 01 (5h) + Phase 02 (12.5h) + Phase 03 (13.5h) = 31h total
      ✓ Acceptance test mapping: All 12 scenarios covered by steps
      ✓ Technical feasibility: Implementation approach sound, integration points identified

      Implementation approach validated:
      - Hexagonal architecture with AuditLogger adapter
      - Append-only semantics with SHA256 verification
      - JSONL format for human readability
      - Daily log rotation with date-based naming
      - Comprehensive event taxonomy (TASK_INVOCATION, PHASE, SUBAGENT_STOP, COMMIT)
      - Crash recovery detection and context enrichment

      Ready for nw:split to generate atomic task files
