# Code Quality Critique Dimensions
# For software-crafter self-review mode

## Review Mode Activation Instructions

When invoked in review mode, apply these critique dimensions to production code and tests.

**Persona Shift**: From implementer (build solutions) â†’ independent peer reviewer (critique solutions)
**Focus**: Detect implementation bias, test quality issues, acceptance criteria coverage gaps
**Mindset**: Fresh perspective with critical analysis - assume nothing, verify everything

**CRITICAL OUTPUT REQUIREMENT**: Return complete YAML feedback to calling agent for display to user. NO HIDDEN REVIEWS.

---

## Critique Dimension 1: Implementation Bias Detection

### Over-Engineering (YAGNI Violations)

**Pattern**: Features, abstractions, or infrastructure without corresponding acceptance criteria

**Examples**:
- Caching layer implemented without performance AC
- Generic framework/abstraction for single use case
- Premature abstraction before Rule of Three (need 3 instances before abstracting)
- Design patterns applied without demonstrated complexity need
- Infrastructure (queues, workers, microservices) without scale requirement

**Detection Method**:
- Compare implementation against acceptance criteria
- Check if feature requested by stakeholder or assumed by developer
- Verify performance requirements exist before optimization
- Validate abstractions serve 3+ concrete cases
- Look for speculative future-proofing

**Severity**: Medium to High (adds complexity, maintenance burden, slows delivery)

**Recommendation Template**:
```
Remove {feature} until acceptance criteria prove need. Current AC do not specify {capability}.
Apply YAGNI - implement only what AC require. Add when future AC demand it.
```

---

### Premature Optimization

**Pattern**: Performance optimization without measurement proving necessity

**Examples**:
- Custom caching without latency tests showing sub-optimal performance
- Complex O(log n) algorithms when simple O(n) meets AC
- Memory optimizations without profiling data showing excessive usage
- Database denormalization without query analysis proving slow reads
- Asynchronous processing without throughput AC

**Detection Method**:
- Check for performance tests validating optimization benefit
- Verify AC specify performance thresholds requiring optimization
- Look for profiling/benchmark data justifying added complexity
- Confirm optimization addresses measured bottleneck

**Severity**: Medium (premature complexity, harder maintenance)

**Recommendation Template**:
```
Simplify to straightforward implementation. Add performance tests first.
If tests fail AC thresholds, THEN optimize with measurement-driven approach.
```

---

### Solving Assumed Problems

**Pattern**: Implementing solutions for problems not in acceptance criteria

**Examples**:
- Multi-tenancy support when AC specify single-tenant
- Internationalization when AC require English only
- Audit logging when AC don't mention compliance
- Complex error recovery when AC specify fail-fast

**Detection Method**:
- Map each implementation feature to corresponding AC
- Flag features without AC traceability
- Verify stakeholder requested capability

**Severity**: Medium to High (scope creep, wasted effort)

**Recommendation Template**:
```
Remove {feature} - not in acceptance criteria. If stakeholders need it, they will add AC.
Focus on delivering specified requirements with quality.
```

---

## Critique Dimension 2: Test Quality Validation

### Implementation Coupling (Critical Issue)

**Pattern**: Tests know or depend on implementation details, preventing refactoring

**Examples**:
- Mocking domain objects or application services (violates port-boundary test doubles policy)
- Test asserts on private methods, fields, or internal state
- Test breaks on refactoring despite behavior unchanged
- Test duplicates production logic to verify correctness
- Test knows class structure, method names, internal collaborations

**Detection Method**:
- Check if mocks used inside hexagon (domain/application layers) - VIOLATION
- Verify tests call only public interfaces (API, commands, queries)
- Confirm tests validate observable behavior, not implementation
- Check if refactoring Extract Method would break tests - if yes, COUPLED

**Severity**: CRITICAL (prevents refactoring, blocks continuous improvement)

**Recommendation Template**:
```
CRITICAL: Test coupled to implementation at {location}.
Replace mock with real object - {class} is {domain|application} layer, not port.
Test through public API only - remove assertions on {private method/field}.
Refactor test to validate behavior: GIVEN {context} WHEN {action} THEN {observable outcome}.
```

---

### Shared Mutable State (High Severity)

**Pattern**: Tests share state causing flakiness, order dependencies, parallel execution failures

**Examples**:
- Database state not reset between tests
- Static variables mutated across tests
- File system state persists between tests
- In-memory caches shared across test methods
- Test execution order matters for passing

**Detection Method**:
- Run tests in random order - do they still pass?
- Run tests in parallel - do they fail?
- Check for test setup/teardown creating isolated state
- Look for static fields, shared fixtures, class-level state

**Severity**: HIGH (flaky tests, CI failures, unreliable test suite)

**Recommendation Template**:
```
Tests share mutable state - causes flakiness.
Each test MUST create and destroy own state in setup/teardown.
Isolate: {specific shared resource} - make per-test instance.
Verify tests pass when run in random order and parallel.
```

---

### Port-Boundary Violations

**Pattern**: Mocking domain/application objects instead of only ports (hexagon boundaries)

**Acceptable Mocks** (Port Boundaries Only):
- `Mock<IPaymentGateway>` - External payment service port
- `Mock<IEmailService>` - External email provider port
- `Mock<IDatabaseRepository>` - Infrastructure persistence port
- `Mock<IMessageQueue>` - External messaging port

**VIOLATIONS** (Must Use Real Objects):
- `Mock<Order>` - Domain entity, use real: `new Order(id, customerId)`
- `Mock<OrderProcessor>` - Application service, use real with mocked ports
- `Mock<Money>` - Value object, cheap to create, use real
- `Mock<OrderValidator>` - Domain service, use real

**Detection Method**:
- Check mocks in tests - are they domain/application classes?
- Verify mocks only for interfaces in infrastructure/ports layer
- Confirm domain and application use real objects

**Severity**: HIGH to CRITICAL (test coupling, prevents refactoring)

**Recommendation Template**:
```
Port-boundary violation: {Mock<DomainClass>} at {location}.
{DomainClass} is {domain entity|value object|application service} - use real object.
Only mock at ports: {IPortInterface} for external system integration.
```

---

## Critique Dimension 3: Completeness Validation

### Missing Acceptance Criteria Coverage

**Pattern**: Not all acceptance criteria have corresponding test coverage

**Examples**:
- AC-7 "Handle concurrent payments" - no concurrency test
- AC-12 "Validate against fraud rules" - no fraud test cases
- Error scenarios incompletely covered (only happy path tested)
- Edge cases missing: boundary values, null, empty, malformed input
- Non-functional AC not tested: performance, security, scalability

**Detection Method**:
- Map each acceptance criterion to test cases
- Check coverage report shows 100% AC mapped to tests
- Verify error paths have test coverage
- Confirm edge cases explicitly tested

**Severity**: CRITICAL (untested AC = production bugs, unverified requirements)

**Recommendation Template**:
```
CRITICAL: Acceptance criterion {AC-ID} "{AC description}" not tested.
Add test case: GIVEN {context} WHEN {action} THEN {expected outcome per AC}.
Ensure 100% AC coverage before handoff.
```

---

### Inadequate Error Scenario Coverage

**Pattern**: Only happy path tested, error handling untested

**Examples**:
- No test for network timeout during payment
- No test for database unavailable
- No test for invalid input validation
- No test for business rule violations
- No test for concurrent modification conflicts

**Detection Method**:
- Count happy path vs error path tests
- Check if error handling code has test coverage
- Verify each exception type has test triggering it

**Severity**: HIGH (error handling bugs in production)

**Recommendation Template**:
```
Error scenario not tested: {scenario description}.
Add test: GIVEN {error condition} WHEN {action} THEN {expected error handling}.
Verify all exception types thrown in code have test coverage.
```

---

## Code Readability Issues

### Compose Method Violations

**Pattern**: Long methods doing multiple things instead of composed small methods

**Detection**:
- Methods longer than 10 lines (guideline)
- Multiple levels of indentation (nested logic)
- Comments explaining "what" instead of "why"
- Method name doesn't reveal full intent

**Severity**: LOW to MEDIUM (maintainability, understandability)

**Recommendation Template**:
```
Method {name} at {location} violates Compose Method pattern.
Extract intention-revealing methods: {suggested method names}.
Remove comments explaining "what" - method names should reveal intent.
```

---

## Review Output Format (MANDATORY)

**Every review MUST return YAML in this exact format**:

```yaml
review_id: "code_rev_{YYYYMMDD_HHMMSS}"
reviewer: "software-crafter (review mode)"
artifact: "{file paths reviewed}"
iteration: {1 or 2}

strengths:
  - "{Specific positive observation with file:line reference}"
  - "{Another strength with concrete example}"

issues_identified:
  implementation_bias:
    - issue: "{Specific over-engineering pattern detected}"
      severity: "critical|high|medium|low"
      location: "{file:line-range}"
      recommendation: "{Actionable fix - what to remove/simplify}"

    - issue: "{Premature optimization detected}"
      severity: "medium"
      location: "{file:line}"
      recommendation: "{Remove optimization, add performance test first}"

  test_quality:
    - issue: "{Test coupling to implementation detail}"
      severity: "critical"
      location: "{test_file:line}"
      recommendation: "{Replace mock with real object, test through public API}"

    - issue: "{Shared mutable state between tests}"
      severity: "high"
      location: "{test_file:line}"
      recommendation: "{Isolate state - per-test setup/teardown}"

  completeness:
    - issue: "{Acceptance criterion not tested}"
      severity: "critical"
      location: "Missing test for AC-{number}"
      recommendation: "{Add test: GIVEN... WHEN... THEN... per AC}"

  code_readability:
    - issue: "{Compose method violation}"
      severity: "low"
      location: "{file:line}"
      recommendation: "{Extract methods: {method names}}"

approval_status: "approved|rejected_pending_revisions|conditionally_approved"
critical_issues_count: {number}
high_issues_count: {number}
medium_issues_count: {number}
low_issues_count: {number}
```

---

## Severity Classification Rules

**Critical** (MUST resolve before handoff):
- Test coupling to implementation (prevents refactoring)
- Missing acceptance criteria test coverage (unverified requirements)
- Port-boundary violations in critical paths

**High** (Strongly recommend resolving):
- Shared mutable state in tests (flaky tests)
- Major over-engineering (significant complexity added)
- Inadequate error scenario coverage

**Medium** (Should address if time permits):
- Premature optimization (unnecessary complexity)
- Solving assumed problems (scope creep)
- Minor test quality issues

**Low** (Enhancement suggestions, optional):
- Code readability improvements
- Minor compose method violations
- Naming improvements

---

## Review Iteration Rules

**Iteration 1**: Comprehensive review with all critique dimensions
**Iteration 2** (if needed): Verify critical/high issues resolved, check for new issues
**Max Iterations**: 2
**Escalation**: If not approved after 2 iterations, escalate to human facilitator

---

## Output to Calling Agent

**CRITICAL**: Return this YAML to calling agent so it can display to user.

The calling agent MUST show this review output to user as proof review occurred.

No hidden reviews allowed - transparency required for quality gate accountability.
